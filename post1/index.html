<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="author" content="Alen Peric" />
    <meta
      name="description"
      content="How I wrestled vLLM onto an RTX 5070 Ti running Debian 12 (CUDA 12.8) — a chatty, step‑by‑step story plus a ready‑to‑clone GitHub repo."
    />
    <meta
      name="keywords"
      content="vLLM, PyTorch, CUDA 12.8, Blackwell GPU, RTX 5070 Ti, Debian 12, FlashInfer, AWQ, FP4, Alen Peric"
    />
    <title>vLLM on Debian 12 &amp; RTX 5070 Ti — My Sleepless‑Night Guide</title>
    <link rel="icon" type="image/x-icon" href="../img/favicon.svg" />
    <style>
      /* —— keep your original styling —— */
      html,body{height:100%;margin:0;padding:0;}
      canvas.scene{position:fixed;top:0;left:0;width:100%;height:100%;z-index:-1;background:#121212;}
      body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;color:#fff;overflow-x:hidden;background:#121212;display:flex;flex-direction:column;min-height:100vh;}
      .logo{margin:20px auto;font-size:42px;text-align:center;white-space:nowrap;}
      .navbar{display:none;}
      .blog-content{flex:1;margin:20px auto;width:90%;max-width:800px;background:rgba(0,0,0,.7);padding:30px;border-radius:10px;}
      .blog-post h2{margin:0 0 10px;font-size:24px;}
      .blog-post p{margin:0 0 10px;font-size:16px;line-height:1.6;}
      .blog-post code{background:#1e1e1e;color:#7cf;border-radius:4px;padding:2px 4px;font-family:monospace;}
      .blog-post a{color:#1e90ff;text-decoration:none;font-weight:bold;}
      .blog-post a:hover{text-decoration:underline;}
      .social-contact{display:flex;flex-direction:column;align-items:center;gap:10px;margin-bottom:20px;}
      .social-contact a.homepage{font-size:18px;color:#fff;text-decoration:none;}
      .contact-links{display:flex;align-items:center;justify-content:center;gap:20px;flex-wrap:nowrap;}
      .social-contact a.linkedin,.social-contact a.github{color:#fff;text-decoration:none;display:flex;align-items:center;}
      .social-contact img{width:68px;height:auto;}
      .social-contact a.contact,.social-contact a.number{color:#fff;text-decoration:none;font-size:18px;display:flex;align-items:center;gap:5px;}
      @media screen and (max-width:800px){
        .logo{font-size:24px;margin:10px auto;}
        .blog-content{margin:10px auto;padding:20px;}
        .blog-post h2{font-size:20px;}
        .blog-post p{font-size:14px;}
        .social-contact a.homepage{font-size:16px;}
        .contact-links{gap:10px;}
        .social-contact img{width:102px;}
        .social-contact a.contact,.social-contact a.number{font-size:16px;}
      }
    </style>
    <!-- keep existing asset imports if you use them -->
    <script type="module" crossorigin src="../assets/index-b7d38c76.js"></script>
    <link rel="stylesheet" href="../assets/index-5884692c.css" />
  </head>
  <body>
    <canvas class="scene"></canvas>

    <div class="logo">vLLM on Debian 12 &amp; RTX 5070 Ti</div>

    <main>
      <div class="blog-content">
        <div class="blog-post">
          <h2>From “why won’t this compile?!” to 65 tokens/sec</h2>

          <p>
            <em>TL;DR:</em> I spent <strong>3½ sleepless days</strong> forcing <code>vLLM</code> to run on a shiny
            new <strong>RTX 5070 Ti</strong> (Blackwell) under Debian 12. PyTorch <code>2.8‑nightly</code> isn’t
            “officially” ready for <code>sm_120</code>, CUDA 12.8, or AVX‑512 — so I rolled my own build,
            baked a custom Docker image with FlashInfer, and documented everything so you don’t have to
            repeat the pain.
          </p>

          <p>
            Grab the repo here → <a href="https://github.com/alenperic/Debian-PyTorch-RTX5070Ti" target="_blank">github.com/alenperic/Debian‑PyTorch‑RTX5070Ti</a>
          </p>

          <h3>🌙 The long nights, in bullet points</h3>
          <ul>
            <li>
              <strong>PyTorch + CUDA 12.8</strong> — no wheels, so I cloned <code>pytorch</code>,
              exported <code>TORCH_CUDA_ARCH_LIST="12.0"</code>, enabled AVX‑512, and compiled a wheel
              with <code>MAX_JOBS=$(nproc)</code>.
            </li>
            <li>
              <strong>FlashInfer</strong> — cloned and installed with
              <code>pip install -e . -v</code> to get lightning‑fast attention kernels.
            </li>
            <li>
              <strong>Docker build</strong> — based on
              <code>nvcr.io/nvidia/pytorch:25.03‑py3</code>, plus ccache, bitsandbytes, and my fresh
              PyTorch wheel.
            </li>
            <li>
              <strong>Model zoo</strong> — tested <code>DeepSeek‑R1‑FP4</code>,
              <code>WhiteRabbitNeo‑13B‑AWQ</code>, and the monstrous
              <code>Qwen/QwQ‑32B‑AWQ</code>.
            </li>
            <li>
              <strong>Runtime flags</strong> — <code>--gpu-memory-utilization 0.9</code>,
              <code>--enable-chunked-prefill</code>, <code>--cpu-offload-gb 4</code>, and
              <code>--enable-reasoning</code> (because why not let the model call tools?).
            </li>
          </ul>

          <h3>🚀 Quick‑start (super condensed)</h3>
          <ol>
            <li>Install NVIDIA driver ≥ 545 and CUDA 12.8 on Debian 12.</li>
            <li>
              <code>git clone --recursive https://github.com/pytorch/pytorch</code>,
              export the env vars in the README, then
              <code>python setup.py bdist_wheel</code>.
            </li>
            <li>
              Build the Docker image:<br />
              <code>docker build -t vllm-cu128 -f Dockerfile .</code>
            </li>
            <li>
              Pull a model:<br />
              <code>huggingface-cli download Qwen/QwQ-32B-AWQ --local-dir /models/QwQ</code>
            </li>
            <li>
              Run vLLM:<br />
              <code>
                docker run --gpus all -p 8000:8000 -v /models:/models vllm-cu128 \
                python3 -m vllm.entrypoints.api_server --model /models/QwQ \
                --quantization awq --gpu-memory-utilization 0.9 --max-model-len 32768
              </code>
            </li>
          </ol>

          <h3>🤔 Why bother?</h3>
          <p>
            Because Blackwell GPUs are <em>fast</em>, but waiting for “official support” feels like
            watching paint dry. By rolling your own stack you get full FP4/AWQ goodness today,
            <strong>~65 tokens/sec</strong> on a single consumer card, and bragging rights on Discord.
          </p>

          <h3>📚 Full guide &amp; copy‑paste commands</h3>
          <p>
            Everything — from blacklisting <code>nouveau</code> to saving a snapshot tarball of the
            final container — lives in the README of the repo. Clone, skim, run, and shoot me a PR if
            you spot a typo.
          </p>

          <p style="text-align:center;">
            <a href="https://github.com/alenperic/Debian-PyTorch-RTX5070Ti" target="_blank" class="read-more">
              ⭐ View on GitHub
            </a>
          </p>

          <p><em>Happy hacking, and may your VRAM be ever plentiful!</em></p>

          <a href="../" class="read-more">← Back to Blog</a>
        </div>
      </div>
    </main>

    <div class="social-contact">
      <a href="https://alenperic.com" target="_blank" class="homepage">Homepage</a>

      <div class="contact-links">
        <a href="https://www.linkedin.com/in/alen-peric/" target="_blank" class="linkedin" aria-label="LinkedIn Profile">
          <img src="../img/linkedin-icon.png" alt="LinkedIn Profile Icon" />
        </a>
        <a href="https://github.com/alenperic" target="_blank" class="github" aria-label="GitHub Profile">
          <img src="../img/github-icon.png" alt="GitHub Profile Icon" />
        </a>
        <a href="mailto:alenperic@protonmail.com" class="contact">📧 Contact</a>
        <a href="tel:5485881420" class="number">📱 Number</a>
      </div>
    </div>
  </body>
</html>
