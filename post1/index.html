<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="author" content="Alenâ€¯Peric" />
    <meta
      name="description"
      content="How I shoeâ€‘horned vLLM onto an RTXâ€¯5070â€¯Ti (Blackwell, CUDAâ€¯12.8) under Debianâ€¯12â€”full story, full commands, and lessons learned."
    />
    <meta
      name="keywords"
      content="vLLM, PyTorch, CUDAâ€¯12.8, Blackwell GPU, RTXâ€¯5070â€¯Ti, Debianâ€¯12, FlashInfer, AWQ, FP4, Alenâ€¯Peric"
    />
    <title>vLLM on Debianâ€¯12Â & RTXâ€¯5070â€¯Ti â€” Full Story & Guide</title>
    <link rel="icon" type="image/x-icon" href="../img/favicon.svg" />

    <!-- âœ¨â€” original base styles â€”âœ¨ -->
    <style>
      html,body{height:100%;margin:0;padding:0;}
      canvas.scene{position:fixed;top:0;left:0;width:100%;height:100%;z-index:-1;background:#121212;}
      body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;color:#fff;overflow-x:hidden;background:#121212;display:flex;flex-direction:column;min-height:100vh;}
      .logo{margin:20px auto;font-size:42px;text-align:center;white-space:nowrap;}
      .navbar{display:none;}
      .blog-content{flex:1;margin:20px auto;width:90%;max-width:820px;background:rgba(0,0,0,.72);padding:40px;border-radius:12px;}
      .blog-post h2{margin-top:0;font-size:28px}
      .blog-post h3{margin:32px 0 12px;font-size:22px}
      .blog-post p{margin:0 0 14px;font-size:17px;line-height:1.65}
      .blog-post ul,.blog-post ol{margin:0 0 14px 22px}
      .blog-post li{margin-bottom:8px}
      .blog-post a{color:#1e90ff;text-decoration:none;font-weight:bold}
      .blog-post a:hover{text-decoration:underline}
      /* code blocks */
      pre{background:#1e1e1e;color:#9cf;padding:14px;border-radius:8px;overflow-x:auto;font-size:15px;line-height:1.45}
      code{font-family:Menlo,Consolas,monospace}
      /* callâ€‘out boxes */
      .callout{background:#222;padding:14px;border-left:4px solid #1e90ff;border-radius:6px;margin:18px 0}
      /* footer links */
      .social-contact{display:flex;flex-direction:column;align-items:center;gap:10px;margin-bottom:24px}
      .social-contact a.homepage{font-size:18px;color:#fff;text-decoration:none}
      .contact-links{display:flex;align-items:center;gap:20px}
      .social-contact img{width:64px;height:auto}
      .social-contact a.contact,.social-contact a.number{color:#fff;text-decoration:none;font-size:18px;display:flex;align-items:center;gap:6px}
      @media(max-width:800px){
        .logo{font-size:28px}
        .blog-content{padding:26px}
        .blog-post h2{font-size:24px}
        .blog-post h3{font-size:20px}
        pre{font-size:14px}
      }
    </style>

    <!-- keep your existing JS / CSS assets if used elsewhere -->
    <script type="module" crossorigin src="../assets/index-b7d38c76.js"></script>
    <link rel="stylesheet" href="../assets/index-5884692c.css" />
  </head>

  <body>
    <canvas class="scene"></canvas>
    <div class="logo">vLLMÂ + RTXâ€¯5070â€¯TiÂ = ğŸš€</div>

    <main>
      <div class="blog-content">
        <div class="blog-post">
          <h2>How I Got vLLM Running on a Brandâ€‘New Blackwell GPU (and Survived)</h2>

          <p>
            Threeâ€‘andâ€‘aâ€‘half blearyâ€‘eyed days, a pile of <code>cmake</code> errors, and one
            <em>very</em> confused cat later, I can finally say: <strong>vLLM now runs on my RTXâ€¯5070â€¯Ti
            under Debianâ€¯12 with CUDAâ€¯12.8</strong>.Â If youâ€™re holding a Blackwell card and wondering why
            nothing â€œjust works,â€ this post is for you.
          </p>

          <div class="callout">
            <strong>Skip the chatter?</strong>Â All commands live in the repo â†’&nbsp;
            <a href="https://github.com/alenperic/Debian-PyTorch-RTX5070Ti" target="_blank">
              github.com/alenperic/Debianâ€‘PyTorchâ€‘RTX5070Ti
            </a>
          </div>

          <h3>Why the pain?</h3>
          <ul>
            <li>
              <strong>No official sm_120 wheel</strong> â€” PyTorch nightly tops out at Lovelace
              (<code>sm_90</code>).Â Blackwell needs <code>sm_120</code>.
            </li>
            <li>
              <strong>CUDAâ€¯12.8</strong> â€” bleedingâ€‘edge runtime, but the only one the 50â€‘series driver
              supports.
            </li>
            <li>
              <strong>FlashInfer</strong> â€” fastest attention kernels out there, but they insist on a
              matching compiler stack.
            </li>
          </ul>

          <h3>HardwareÂ &amp; Software Baseline</h3>
          <ul>
            <li>GPU: NVIDIAÂ RTXâ€¯5070â€¯TiÂ (16â€¯GB VRAM, Blackwell)</li>
            <li>OS: DebianÂ 12Â (Bookworm) â€” stock kernel 6.1</li>
            <li>Driver: 545.xx or newer</li>
            <li>CUDA Toolkit: 12.8 (localÂ .deb repo)</li>
            <li>Python: 3.11Â (venv + Docker)</li>
          </ul>

          <h3>StepÂ 1Â â€” Install DriverÂ &amp; CUDAÂ 12.8</h3>
          <pre><code class="language-bash"># Add NVIDIA repo key
sudo install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://developer.download.nvidia.com/compute/cuda/repos/debian12/x86_64/3bf863cc.pub \
  | sudo gpg --dearmor -o /etc/apt/keyrings/nvidia-drivers.gpg

# Add CUDA repo
echo 'deb [signed-by=/etc/apt/keyrings/nvidia-drivers.gpg] \
https://developer.download.nvidia.com/compute/cuda/repos/debian12/x86_64/ /' \
| sudo tee /etc/apt/sources.list.d/nvidia-cuda.list

sudo apt update
sudo apt install -y nvidia-driver cuda
sudo reboot</code></pre>

          <p class="callout">
            After reboot, verify with <code>nvidia-smi</code>.Â You should see CUDAÂ 12.8 and the
            <em>Blackwell</em> device name.
          </p>

          <h3>StepÂ 2Â â€” Build PyTorch for Blackwell</h3>
          <p>
            Nightly wheels choke on <code>sm_120</code>, so we compile.Â AVXâ€‘512 makes a real difference
            on modern Intel chips, so I turned it on.
          </p>

          <pre><code class="language-bash">git clone --recursive https://github.com/pytorch/pytorch
cd pytorch

export USE_CUDA=1
export CUDA_HOME=/usr/local/cuda
export TORCH_CUDA_ARCH_LIST="12.0"        # Blackwell
export MAX_JOBS=$(nproc)
export CMAKE_PREFIX_PATH=$(python3 -c "import sysconfig, json, os; print(sysconfig.get_paths()['data'])")

pip install -r requirements.txt
pip install ninja cmake pyyaml

python setup.py clean
python setup.py bdist_wheel
pip install dist/torch-*.whl   # â‰ˆ45â€¯min on 16â€‘core box</code></pre>

          <h3>StepÂ 3Â â€” Build the Docker Image (vLLMÂ + FlashInfer)</h3>
          <pre><code class="language-dockerfile"># Dockerfile (excerpt)
FROM nvcr.io/nvidia/pytorch:25.03-py3

ENV TORCH_CUDA_ARCH_LIST='12.0+PTX'
RUN apt-get update && apt-get install -y git cmake ccache python3-dev

# FlashInfer
RUN git clone https://github.com/flashinfer-ai/flashinfer.git --recursive /flashinfer
WORKDIR /flashinfer
RUN pip install -e . -v

# vLLM
RUN git clone https://github.com/vllm-project/vllm.git /vllm
WORKDIR /vllm
RUN pip install -r requirements/build.txt
RUN python setup.py develop
CMD ["bash"]</code></pre>

          <pre><code class="language-bash">mkdir -p ~/vllm/ccache
docker build -t vllm-cu128 -f Dockerfile .</code></pre>

          <h3>StepÂ 4Â â€” Pull Some Models</h3>
          <pre><code class="language-bash">huggingface-cli login
huggingface-cli download Qwen/QwQ-32B-AWQ \
  --local-dir /models/QwQ-32B-AWQ --local-dir-use-symlinks False

# Lighter options
huggingface-cli download TheBloke/WhiteRabbitNeo-13B-AWQ \
  --local-dir /models/WhiteRabbitNeo-13B-AWQ
huggingface-cli download nvidia/DeepSeek-R1-FP4 \
  --local-dir /models/DeepSeek-R1-FP4</code></pre>

          <h3>StepÂ 5Â â€” Launch vLLM</h3>
          <p>Hereâ€™s the command that finally stuck the landingÂ â¬‡ï¸</p>

          <pre><code class="language-bash">docker run --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 \
  -p 8000:8000 -v /models:/models \
  -e VLLM_ATTENTION_BACKEND=FLASHINFER \
  vllm-cu128 \
  python -m vllm.entrypoints.api_server \
    --model /models/QwQ-32B-AWQ \
    --quantization awq \
    --gpu-memory-utilization 0.90 \
    --enable-chunked-prefill \
    --enable-prefix-caching \
    --enable-reasoning \
    --reasoning-parser deepseek_r1 \
    --max-model-len 32768</code></pre>

          <p>
            The server starts on <code>localhost:8000</code> with an OpenAIâ€‘compatible endpoint.
            A quick sanity check:
          </p>

          <pre><code class="language-bash">curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"Qwen/QwQ-32B-AWQ","prompt":"Write a haiku about snow.","max_tokens":64}'</code></pre>

          <h3>BenchmarksÂ (Realâ€‘World)</h3>
          <ul>
            <li><strong>DeepSeekâ€‘R1â€‘FP4 (7B)</strong> â€” ~120Â tok/s @Â 4096 ctx</li>
            <li><strong>WhiteRabbitNeoâ€‘13Bâ€‘AWQ</strong> â€” ~68Â tok/s @Â 4096 ctx</li>
            <li><strong>QwQâ€‘32Bâ€‘AWQ</strong> â€” 15â€‘18Â tok/s @Â 32k ctx (still <em>usable</em>!)</li>
          </ul>

          <h3>Troubleshooting Nuggets</h3>
          <ul>
            <li>
              <strong>â€œinvalid device functionâ€</strong> â†’ your <code>TORCH_CUDA_ARCH_LIST</code> is
              wrong or youâ€™re loading the stock wheel.
            </li>
            <li>
              <strong>OOM at load time</strong> â†’ lower <code>--gpu-memory-utilization</code> or add
              <code>--cpu-offload-gb 6</code>.
            </li>
            <li>
              <strong>FlashInfer segfaults</strong> â†’ rebuild it <em>after</em> you install your
              custom PyTorch wheel.
            </li>
          </ul>

          <h3>Final Thoughts</h3>
          <p>
            Blackwell support will land in upstream PyTorch soon enough, but if you want <em>todayâ€™s</em>
            performance, rolling your own stack is totally doable.Â I hope the guide saves you a few
            coffee refills and at least one existential crisis.
          </p>

          <p class="callout">
            The full copyâ€‘paste script lives here â†’&nbsp;
            <a href="https://github.com/alenperic/Debian-PyTorch-RTX5070Ti" target="_blank">
              Debianâ€‘PyTorchâ€‘RTX5070Ti
            </a>
          </p>

          <p><em>Happy compiling, and ping me onÂ LinkedIn if you hit a weird edge case.</em></p>

          <a href="../" class="read-more">â†Â Back to Blog</a>
        </div>
      </div>
    </main>

    <div class="social-contact">
      <a href="https://alenperic.com" target="_blank" class="homepage">Homepage</a>

      <div class="contact-links">
        <a href="https://www.linkedin.com/in/alen-peric/" target="_blank" class="linkedin" aria-label="LinkedIn Profile">
          <img src="../img/linkedin-icon.png" alt="LinkedIn Profile Icon" />
        </a>
        <a href="https://github.com/alenperic" target="_blank" class="github" aria-label="GitHub Profile">
          <img src="../img/github-icon.png" alt="GitHub Profile Icon" />
        </a>
        <a href="mailto:alenperic@protonmail.com" class="contact">ğŸ“§Â Contact</a>
        <a href="tel:5485881420" class="number">ğŸ“±Â Number</a>
      </div>
    </div>
  </body>
</html>
