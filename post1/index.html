<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="author" content="Alenâ€¯Peric" />
    <meta
      name="description"
      content="How I wrestled vLLM onto an RTXâ€¯5070â€¯Ti running Debianâ€¯12 (CUDAâ€¯12.8) â€” a chatty, stepâ€‘byâ€‘step story plus a readyâ€‘toâ€‘clone GitHub repo."
    />
    <meta
      name="keywords"
      content="vLLM, PyTorch, CUDAâ€¯12.8, Blackwell GPU, RTXâ€¯5070â€¯Ti, Debianâ€¯12, FlashInfer, AWQ, FP4, Alenâ€¯Peric"
    />
    <title>vLLM on Debianâ€¯12 &amp; RTXâ€¯5070â€¯Ti â€” My Sleeplessâ€‘Night Guide</title>
    <link rel="icon" type="image/x-icon" href="../img/favicon.svg" />
    <style>
      /* â€”â€” keep your original styling â€”â€” */
      html,body{height:100%;margin:0;padding:0;}
      canvas.scene{position:fixed;top:0;left:0;width:100%;height:100%;z-index:-1;background:#121212;}
      body{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;color:#fff;overflow-x:hidden;background:#121212;display:flex;flex-direction:column;min-height:100vh;}
      .logo{margin:20px auto;font-size:42px;text-align:center;white-space:nowrap;}
      .navbar{display:none;}
      .blog-content{flex:1;margin:20px auto;width:90%;max-width:800px;background:rgba(0,0,0,.7);padding:30px;border-radius:10px;}
      .blog-post h2{margin:0 0 10px;font-size:24px;}
      .blog-post p{margin:0 0 10px;font-size:16px;line-height:1.6;}
      .blog-post code{background:#1e1e1e;color:#7cf;border-radius:4px;padding:2px 4px;font-family:monospace;}
      .blog-post a{color:#1e90ff;text-decoration:none;font-weight:bold;}
      .blog-post a:hover{text-decoration:underline;}
      .social-contact{display:flex;flex-direction:column;align-items:center;gap:10px;margin-bottom:20px;}
      .social-contact a.homepage{font-size:18px;color:#fff;text-decoration:none;}
      .contact-links{display:flex;align-items:center;justify-content:center;gap:20px;flex-wrap:nowrap;}
      .social-contact a.linkedin,.social-contact a.github{color:#fff;text-decoration:none;display:flex;align-items:center;}
      .social-contact img{width:68px;height:auto;}
      .social-contact a.contact,.social-contact a.number{color:#fff;text-decoration:none;font-size:18px;display:flex;align-items:center;gap:5px;}
      @media screen and (max-width:800px){
        .logo{font-size:24px;margin:10px auto;}
        .blog-content{margin:10px auto;padding:20px;}
        .blog-post h2{font-size:20px;}
        .blog-post p{font-size:14px;}
        .social-contact a.homepage{font-size:16px;}
        .contact-links{gap:10px;}
        .social-contact img{width:102px;}
        .social-contact a.contact,.social-contact a.number{font-size:16px;}
      }
    </style>
    <!-- keep existing asset imports if you use them -->
    <script type="module" crossorigin src="../assets/index-b7d38c76.js"></script>
    <link rel="stylesheet" href="../assets/index-5884692c.css" />
  </head>
  <body>
    <canvas class="scene"></canvas>

    <div class="logo">vLLM on Debianâ€¯12 &amp; RTXâ€¯5070â€¯Ti</div>

    <main>
      <div class="blog-content">
        <div class="blog-post">
          <h2>From â€œwhy wonâ€™t this compile?!â€ to 65â€¯tokens/sec</h2>

          <p>
            <em>TL;DR:</em> I spent <strong>3Â½ sleepless days</strong> forcing <code>vLLM</code> to run on a shiny
            new <strong>RTXâ€¯5070â€¯Ti</strong> (Blackwell) under Debianâ€¯12. PyTorch <code>2.8â€‘nightly</code> isnâ€™t
            â€œofficiallyâ€ ready for <code>sm_120</code>, CUDAâ€¯12.8, or AVXâ€‘512 â€” so I rolled my own build,
            baked a custom Docker image with FlashInfer, and documented everything so you donâ€™t have to
            repeat the pain.
          </p>

          <p>
            Grab the repo here â†’ <a href="https://github.com/alenperic/Debian-PyTorch-RTX5070Ti" target="_blank">github.com/alenperic/Debianâ€‘PyTorchâ€‘RTX5070Ti</a>
          </p>

          <h3>ğŸŒ™Â The long nights, in bullet points</h3>
          <ul>
            <li>
              <strong>PyTorch + CUDAâ€¯12.8</strong> â€” no wheels, so I cloned <code>pytorch</code>,
              exported <code>TORCH_CUDA_ARCH_LIST="12.0"</code>, enabled AVXâ€‘512, and compiled a wheel
              with <code>MAX_JOBS=$(nproc)</code>.
            </li>
            <li>
              <strong>FlashInfer</strong> â€” cloned and installed with
              <code>pip install -e . -v</code> to get lightningâ€‘fast attention kernels.
            </li>
            <li>
              <strong>Docker build</strong> â€” based on
              <code>nvcr.io/nvidia/pytorch:25.03â€‘py3</code>, plus ccache, bitsandbytes, and my fresh
              PyTorch wheel.
            </li>
            <li>
              <strong>Model zoo</strong> â€” tested <code>DeepSeekâ€‘R1â€‘FP4</code>,
              <code>WhiteRabbitNeoâ€‘13Bâ€‘AWQ</code>, and the monstrous
              <code>Qwen/QwQâ€‘32Bâ€‘AWQ</code>.
            </li>
            <li>
              <strong>Runtime flags</strong> â€” <code>--gpu-memory-utilization 0.9</code>,
              <code>--enable-chunked-prefill</code>, <code>--cpu-offload-gb 4</code>, and
              <code>--enable-reasoning</code> (because why not let the model call tools?).
            </li>
          </ul>

          <h3>ğŸš€Â Quickâ€‘start (super condensed)</h3>
          <ol>
            <li>Install NVIDIA driverÂ â‰¥Â 545 and CUDAâ€¯12.8 on Debianâ€¯12.</li>
            <li>
              <code>git clone --recursive https://github.com/pytorch/pytorch</code>,
              export the env vars in the README, then
              <code>python setup.py bdist_wheel</code>.
            </li>
            <li>
              Build the Docker image:<br />
              <code>docker build -t vllm-cu128 -f Dockerfile .</code>
            </li>
            <li>
              Pull a model:<br />
              <code>huggingface-cli download Qwen/QwQ-32B-AWQ --local-dir /models/QwQ</code>
            </li>
            <li>
              Run vLLM:<br />
              <code>
                docker run --gpus all -p 8000:8000 -v /models:/models vllm-cu128 \
                python3 -m vllm.entrypoints.api_server --model /models/QwQ \
                --quantization awq --gpu-memory-utilization 0.9 --max-model-len 32768
              </code>
            </li>
          </ol>

          <h3>ğŸ¤”Â Why bother?</h3>
          <p>
            Because Blackwell GPUs are <em>fast</em>, but waiting for â€œofficial supportâ€ feels like
            watching paint dry. By rolling your own stack you get full FP4/AWQ goodness today,
            <strong>~65â€¯tokens/sec</strong> on a single consumer card, and bragging rights on Discord.
          </p>

          <h3>ğŸ“šÂ Full guide &amp; copyâ€‘paste commands</h3>
          <p>
            Everything â€” from blacklisting <code>nouveau</code> to saving a snapshot tarball of the
            final container â€” lives in the README of the repo. Clone, skim, run, and shoot me a PR if
            you spot a typo.
          </p>

          <p style="text-align:center;">
            <a href="https://github.com/alenperic/Debian-PyTorch-RTX5070Ti" target="_blank" class="read-more">
              â­Â View on GitHub
            </a>
          </p>

          <p><em>Happy hacking, and may your VRAM be ever plentiful!</em></p>

          <a href="../" class="read-more">â† Back to Blog</a>
        </div>
      </div>
    </main>

    <div class="social-contact">
      <a href="https://alenperic.com" target="_blank" class="homepage">Homepage</a>

      <div class="contact-links">
        <a href="https://www.linkedin.com/in/alen-peric/" target="_blank" class="linkedin" aria-label="LinkedIn Profile">
          <img src="../img/linkedin-icon.png" alt="LinkedIn Profile Icon" />
        </a>
        <a href="https://github.com/alenperic" target="_blank" class="github" aria-label="GitHub Profile">
          <img src="../img/github-icon.png" alt="GitHub Profile Icon" />
        </a>
        <a href="mailto:alenperic@protonmail.com" class="contact">ğŸ“§Â Contact</a>
        <a href="tel:5485881420" class="number">ğŸ“±Â Number</a>
      </div>
    </div>
  </body>
</html>
