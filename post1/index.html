<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="author" content="Alenâ€¯Peric">
<meta name="description" content="How I shoeâ€‘horned vLLM onto an RTXâ€¯5070â€¯Ti (Blackwell, CUDAâ€¯12.8) under Debianâ€¯12â€”full story, full commands, and lessons learned.">
<meta name="keywords" content="vLLM, PyTorch, CUDAâ€¯12.8, Blackwell GPU, RTXâ€¯5070â€¯Ti, Debianâ€¯12, FlashInfer, AWQ, FP4, Alenâ€¯Peric">
<title>vLLM on Debianâ€¯12â€¯& RTXâ€¯5070â€¯Ti â€” My Sleeplessâ€‘Night Guide</title>
<link rel="icon" type="image/x-icon" href="../img/favicon.svg">
<link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:wght@400;600&display=swap" rel="stylesheet">
<style>
html,body{height:100%;margin:0;padding:0}
canvas.scene{position:fixed;top:0;left:0;width:100%;height:100%;z-index:-1;background:#121212}
body{font-family:Helvetica !important;font-weight:300 !important;color:#fff;overflow-x:hidden;background:#121212;display:flex;flex-direction:column;min-height:100vh}
*{font-family:Helvetica !important;font-weight:300 !important;}
.logo{margin:20px auto 0;max-width:90%;font-size:40px;text-align:center;white-space:normal}
.blog-content{flex:1;margin:100px auto 40px;width:92%;max-width:820px;background:rgba(0,0,0,.72);padding:40px;border-radius:12px}
.blog-post h2{margin-top:0;font-size:28px}
.blog-post h3{margin:32px 0 12px;font-size:22px}
.blog-post p{margin:0 0 14px;font-size:17px;line-height:1.65}
.blog-post ul,.blog-post ol{margin:0 0 14px 22px}
.blog-post li{margin-bottom:8px}
.blog-post a{color:#1e90ff;text-decoration:none;font-weight:bold}
.blog-post a:hover{text-decoration:underline}
pre{background:#1e1e1e;color:#c9e3ff;padding:20px 56px 20px 20px;border-radius:10px;overflow-x:auto;font-size:15px;line-height:1.45;position:relative}
code{font-family:'Source Code Pro',Menlo,Consolas,monospace;font-style:italic;font-weight:400}
.copy-btn{position:absolute;top:12px;right:12px;background:#1e90ff;border:none;color:#fff;padding:4px 12px;font-size:12px;border-radius:4px;cursor:pointer;opacity:.85;transition:opacity .2s;z-index:2}
.copy-btn:hover{opacity:1}
.callout{background:#222;padding:14px;border-left:4px solid #1e90ff;border-radius:6px;margin:18px 0}
.social-contact{margin:0 auto 24px;display:flex;flex-direction:column;align-items:center;gap:10px}
.social-contact a.homepage{font-size:18px;color:#fff;text-decoration:none}
.contact-links{display:flex;align-items:center;gap:20px}
.social-contact img{width:64px;height:auto}
.social-contact a.contact,.social-contact a.number{color:#fff;text-decoration:none;font-size:18px;display:flex;align-items:center;gap:6px}
@media(max-width:800px){
.logo{font-size:28px;margin-top:14px}
.blog-content{margin:86px auto 34px;padding:26px}
.blog-post h2{font-size:24px}
.blog-post h3{font-size:20px}
pre{font-size:14px;padding:18px 46px 18px 18px}
}
@media(max-width:600px){
.logo{font-size:22px;margin-top:10px}
.blog-content{margin:70px auto 30px;padding:20px}
.blog-post h2{font-size:20px}
.blog-post p{font-size:14px}
.social-contact a.homepage{font-size:16px}
.contact-links{gap:14px}
.social-contact img{width:56px}
.social-contact a.contact,.social-contact a.number{font-size:15px}
}
</style>
</head>
<body>
<canvas class="scene"></canvas>
<div class="logo">BlackwellÂ BrawlÂ â€” vLLMÂ meetsÂ RTXâ€¯5070â€¯Ti ğŸ¥ŠğŸš€</div>
<main>
<div class="blog-content">
<div class="blog-post">
<h2>How I Got vLLM Running on a Brandâ€‘New Blackwell GPU (and Survived)</h2>
<p>Threeâ€‘andâ€‘aâ€‘half blearyâ€‘eyed days, a pile of <code>cmake</code> errors, and one <em>very</em> confused cat later, I can finally say: <strong>vLLM now runs on my RTXâ€¯5070â€¯Ti under Debianâ€¯12 with CUDAâ€¯12.8</strong>. If youâ€™re holding a Blackwell card and wondering why nothing â€œjust works,â€ this post is for you.</p>
<div class="callout"><strong>Skip the chatter?</strong> All commands live in the repo â†’ <a href="https://github.com/alenperic/Debian-PyTorch-RTX5070Ti" target="_blank">github.com/alenperic/Debianâ€‘PyTorchâ€‘RTX5070Ti</a></div>
<h3>Why the pain?</h3>
<ul>
<li><strong>No official <code>sm_120</code> wheel</strong> â€” PyTorch nightly tops out at Lovelace (<code>sm_90</code>). Blackwell needs <code>sm_120</code>.</li>
<li><strong>CUDAâ€¯12.8</strong> â€” bleedingâ€‘edge runtime, but the only one the 50â€‘series driver supports.</li>
<li><strong>FlashInfer</strong> â€” fastest attention kernels out there, but they insist on a matching compiler stack.</li>
</ul>
<p>In other words, the perfect storm of â€œit should work in theoryâ€ and â€œwhy is my coffee mug screaming?â€</p>
<h3>HardwareÂ & Software Baseline</h3>
<ul>
<li>GPU: NVIDIAÂ RTXâ€¯5070â€¯TiÂ (16â€¯GB VRAM, Blackwell)</li>
<li>OS: DebianÂ 12Â (Bookworm) â€” stock kernelÂ 6.1</li>
<li>Driver: 545.xx or newer</li>
<li>CUDA Toolkit: 12.8</li>
<li>Python: 3.11Â (venvÂ + Docker)</li>
</ul>
<h3>StepÂ 1Â â€” Install DriverÂ & CUDAâ€¯12.8</h3>
<p>First we exorcise <em>nouveau</em>, add the CUDA repo, and pray we donâ€™t typo the URL.</p>
<pre><code class="language-bash">sudo install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://developer.download.nvidia.com/compute/cuda/repos/debian12/x86_64/3bf863cc.pub \
  | sudo gpg --dearmor -o /etc/apt/keyrings/nvidia-drivers.gpg
echo 'deb [signed-by=/etc/apt/keyrings/nvidia-drivers.gpg] \
https://developer.download.nvidia.com/compute/cuda/repos/debian12/x86_64/ /' \
| sudo tee /etc/apt/sources.list.d/nvidia-cuda.list
sudo apt update
sudo apt install -y nvidia-driver cuda
sudo reboot</code></pre>
<p class="callout">After reboot, verify with <code>nvidia-smi</code>. You should see CUDAâ€¯12.8 and a shiny Blackwell device name. If you donâ€™t, welcome to GroundhogÂ Day â€” repeat until reality matches documentation.</p>
<h3>StepÂ 2Â â€” Build PyTorch for Blackwell</h3>
<p>Nightly wheels choke on <code>sm_120</code>, so we compile. AVXâ€‘512 makes a real difference on modern Intel chips, so I turned it on.</p>
<pre><code class="language-bash">git clone --recursive https://github.com/pytorch/pytorch
cd pytorch
export USE_CUDA=1
export CUDA_HOME=/usr/local/cuda
export TORCH_CUDA_ARCH_LIST="12.0"
export MAX_JOBS=$(nproc)
export CMAKE_PREFIX_PATH=$(python3 -c "import sysconfig; print(sysconfig.get_paths()['data'])")
pip install -r requirements.txt
pip install ninja cmake pyyaml
python setup.py clean
python setup.py bdist_wheel
pip install dist/torch-*.whl</code></pre>
<h3>StepÂ 3Â â€” Build the Docker Image (vLLMÂ + FlashInfer)</h3>
<p>Because nothing says â€œI love repetitionâ€ like compiling <em>again</em> inside a container.</p>
<pre><code class="language-dockerfile">FROM nvcr.io/nvidia/pytorch:25.03-py3
ENV TORCH_CUDA_ARCH_LIST='12.0+PTX'
RUN apt-get update && apt-get install -y git cmake ccache python3-dev
RUN git clone https://github.com/flashinfer-ai/flashinfer.git --recursive /flashinfer
WORKDIR /flashinfer
RUN pip install -e . -v
RUN git clone https://github.com/vllm-project/vllm.git /vllm
WORKDIR /vllm
RUN pip install -r requirements/build.txt
RUN python setup.py develop
CMD ["bash"]</code></pre>
<pre><code class="language-bash">mkdir -p ~/vllm/ccache
docker build -t vllm-cu128 -f Dockerfile .</code></pre>
<h3>StepÂ 4Â â€” Pull Some Models</h3>
<p>Time to hoard weights like a dragon.</p>
<pre><code class="language-bash">huggingface-cli login
huggingface-cli download Qwen/QwQ-32B-AWQ \
  --local-dir /models/QwQ-32B-AWQ --local-dir-use-symlinks False
huggingface-cli download TheBloke/WhiteRabbitNeo-13B-AWQ \
  --local-dir /models/WhiteRabbitNeo-13B-AWQ
huggingface-cli download nvidia/DeepSeek-R1-FP4 \
  --local-dir /models/DeepSeek-R1-FP4</code></pre>
<h3>StepÂ 5Â â€” Launch vLLM</h3>
<p>The moment of truth. If this fails, scroll back to StepÂ 1 and pretend you never saw this line.</p>
<pre><code class="language-bash">docker run --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 \
  -p 8000:8000 -v /models:/models \
  -e VLLM_ATTENTION_BACKEND=FLASHINFER \
  vllm-cu128 \
  python -m vllm.entrypoints.api_server \
    --model /models/QwQ-32B-AWQ \
    --quantization awq \
    --gpu-memory-utilization 0.90 \
    --enable-chunked-prefill \
    --enable-prefix-caching \
    --enable-reasoning \
    --reasoning-parser deepseek_r1 \
    --max-model-len 32768</code></pre>
<p>The server starts on <code>localhost:8000</code>. A quick sanity check:</p>
<pre><code class="language-bash">curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"Qwen/QwQ-32B-AWQ","prompt":"Write a haiku about snow.","max_tokens":64}'</code></pre>
<h3>BenchmarksÂ (Realâ€‘World)</h3>
<ul>
<li><strong><a href="https://huggingface.co/nvidia/DeepSeek-R1-FP4" target="_blank">DeepSeekâ€‘R1â€‘FP4â€¯(7B)</a></strong> â€” ~120Â tok/s @Â 4096Â ctx</li>
<li><strong><a href="https://huggingface.co/TheBloke/WhiteRabbitNeo-13B-AWQ" target="_blank">WhiteRabbitNeoâ€‘13Bâ€‘AWQ</a></strong> â€” ~68Â tok/s @Â 4096Â ctx</li>
<li><strong><a href="https://huggingface.co/Qwen/QwQ-32B-AWQ" target="_blank">QwQâ€‘32Bâ€‘AWQ</a></strong> â€” 15â€‘18Â tok/s @Â 32kÂ ctx</li>
</ul>
<h3>Troubleshooting Nuggets</h3>
<ul>
<li><strong>invalid device function</strong> â†’ your <code>TORCH_CUDA_ARCH_LIST</code> is wrong or youâ€™re loading the stock wheel.</li>
<li><strong>OOM at load time</strong> â†’ lower <code>--gpu-memory-utilization</code> or add <code>--cpu-offload-gbÂ 6</code>.</li>
<li><strong>FlashInfer segfaults</strong> â†’ rebuild it after you install your custom PyTorch wheel.</li>
</ul>
<h3>Final Thoughts</h3>
<p>Blackwell support will land in upstream PyTorch soon enough, but if you want todayâ€™s performance, rolling your own stack is totally doable. I hope the guide saves you a few coffee refills and at least one existential crisis.</p>
<div class="callout">The full copyâ€‘paste script lives here â†’ <a href="https://github.com/alenperic/Debian-PyTorch-RTX5070Ti" target="_blank">Debianâ€‘PyTorchâ€‘RTX5070Ti</a></div>
<p><em>Happy compiling, and ping me onÂ LinkedIn if you hit a weird edge case.</em></p>
<a href="../" class="read-more">â†Â Back to Blog</a>
</div>
</div>
</main>
<div class="social-contact">
<a href="https://alenperic.com" target="_blank" class="homepage">Homepage</a>
<div class="contact-links">
<a href="https://www.linkedin.com/in/alen-peric/" target="_blank" class="linkedin"><img src="../img/linkedin-icon.png" alt="LinkedIn Profile Icon"></a>
<a href="https://github.com/alenperic" target="_blank" class="github"><img src="../img/github-icon.png" alt="GitHub Profile Icon"></a>
<a href="mailto:alenperic@protonmail.com" class="contact">ğŸ“§Â Contact</a>
<a href="tel:5485881420" class="number">ğŸ“±Â Number</a>
</div>
</div>
<script>
document.addEventListener('DOMContentLoaded',()=>{document.querySelectorAll('pre').forEach(pre=>{const btn=document.createElement('button');btn.className='copy-btn';btn.textContent='Copy';pre.appendChild(btn);btn.addEventListener('click',()=>{navigator.clipboard.writeText(pre.innerText).then(()=>{btn.textContent='Copied!';setTimeout(()=>btn.textContent='Copy',1600);});});});});
</script>
</body>
</html>
